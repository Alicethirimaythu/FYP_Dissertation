\begin{thebibliography}{xx}

\harvarditem{{\em akrasia, n.}}{n.d.}{oed:akrasia}
{\em akrasia, n.}  \harvardyearleft n.d.\harvardyearright .
\newline\harvardurl{https://www.oed.com/view/Entry/240257?redirectedFrom=akrasia}

\harvarditem[Ameer et~al.]{Ameer, Bölücü, Siddiqui, Can, Sidorov
  \harvardand\ Gelbukh}{2023}{AMEER2023118534}
Ameer, I., Bölücü, N., Siddiqui, M. H.~F., Can, B., Sidorov, G. \harvardand\
  Gelbukh, A.  \harvardyearleft 2023\harvardyearright , `Multi-label emotion
  classification in texts using transfer learning', {\em Expert Systems with
  Applications} {\bf 213},~118534.
\newline\harvardurl{https://www.sciencedirect.com/science/article/pii/S0957417422016098}

\harvarditem{Hashemi-Pour \harvardand\
  Lutkevich}{2024}{Hashemi-Pour_Lutkevich_2024}
Hashemi-Pour, C. \harvardand\ Lutkevich, B.  \harvardyearleft
  2024\harvardyearright , `What is the bert language model?: Definition from
  techtarget.com'.
\newline\harvardurl{https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection.}

\harvarditem{Iraqi}{2023}{Iraqi_2023}
Iraqi, M.  \harvardyearleft 2023\harvardyearright , `Comparing the performance
  of llms: A deep dive into roberta, llama-2, and mistral-7b for disaster...'.
\newline\harvardurl{https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a}

\harvarditem{Jorge}{2023}{Jorge_2023}
Jorge, L.  \harvardyearleft 2023\harvardyearright , `Roberta vs. gpt: A
  comprehensive comparison of state-of-the-art language models, with expert
  insights from cronj'.
\newline\harvardurl{https://medium.com/@livajorge7/roberta-vs-86ee82a44969#:~:text=Pretraining%20Objectives%3A%20RoBERTa%20is%20pretrained,masked%20words%20in%20a%20sentence.}

\harvarditem{Krugmann \harvardand\ Hartmann}{2024}{Krugmann_Hartmann_2024}
Krugmann, J.~O. \harvardand\ Hartmann, J.  \harvardyearleft
  2024\harvardyearright , `Sentiment analysis in the age of generative ai -
  customer needs and solutions'.
\newline\harvardurl{https://link.springer.com/article/10.1007/s40547-024-00143-4#Tab1}

\harvarditem{Nedilko}{n.d.}{Nedilko}
Nedilko, A.  \harvardyearleft n.d.\harvardyearright , `Generative pretrained
  transformers for emotion detection in a code-switching setting'.
\newline\harvardurl{https://aclanthology.org/2023.wassa-1.61/}

\harvarditem{Pandey}{2021}{Pandey_2021}
Pandey, P.  \harvardyearleft 2021\harvardyearright , `Emotion dataset for
  emotion recognition tasks'.
\newline\harvardurl{https://www.kaggle.com/datasets/parulpandey/emotion-dataset/data}

\harvarditem{patrickvonplaten}{n.d.}{patrickvonplaten}
patrickvonplaten  \harvardyearleft n.d.\harvardyearright ,
  `Microsoft/minilm-l12-h384-uncased · hugging face'.
\newline\harvardurl{https://huggingface.co/microsoft/MiniLM-L12-H384-uncased}

\harvarditem[Saravia et~al.]{Saravia, Liu, Huang, Wu \harvardand\
  Chen}{2018}{saravia-etal-2018-carer}
Saravia, E., Liu, H.-C.~T., Huang, Y.-H., Wu, J. \harvardand\ Chen, Y.-S.
  \harvardyearleft 2018\harvardyearright , {CARER}: Contextualized affect
  representations for emotion recognition, {\em in} `Proceedings of the 2018
  Conference on Empirical Methods in Natural Language Processing', Association
  for Computational Linguistics, Brussels, Belgium, pp.~3687--3697.
\newline\harvardurl{https://www.aclweb.org/anthology/D18-1404}

\harvarditem{Sharma}{2022}{Sharma_2022}
Sharma, D.  \harvardyearleft 2022\harvardyearright , `A gentle introduction to
  roberta'.
\newline\harvardurl{https://www.analyticsvidhya.com/blog/2022/10/a-gentle-introduction-to-roberta/}

\harvarditem{Uwa}{2023}{Uwa_2023}
Uwa  \harvardyearleft 2023\harvardyearright , `Science of emotion: The basics
  of emotional psychology: Uwa'.
\newline\harvardurl{https://online.uwa.edu/news/emotional-psychology/}

\harvarditem[Vaswani et~al.]{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser \harvardand\
  Polosukhin}{2023}{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2023}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L. \harvardand\ Polosukhin, I.  \harvardyearleft
  2023\harvardyearright , `Attention is all you need'.
\newline\harvardurl{https://arxiv.org/abs/1706.03762}

\harvarditem[Wang et~al.]{Wang, Wei, Dong, Bao, Yang \harvardand\
  Zhou}{2020}{Wang_Wei_Dong_Bao_Yang_Zhou_2020}
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N. \harvardand\ Zhou, M.
  \harvardyearleft 2020\harvardyearright , `Minilm: Deep self-attention
  distillation for task-agnostic compression of pre-trained transformers'.
\newline\harvardurl{https://arxiv.org/abs/2002.10957}

\harvarditem{{\em What is natural language processing (NLP)?}}{n.d.}{nlp}
{\em What is natural language processing (NLP)?}  \harvardyearleft
  n.d.\harvardyearright .
\newline\harvardurl{https://www.ibm.com/topics/natural-language-processing}

\harvarditem[Zhang et~al.]{Zhang, Wang, Wu, Tiwari, Li, Wang \harvardand\
  Qin}{2024}{zhang2024dialoguellm}
Zhang, Y., Wang, M., Wu, Y., Tiwari, P., Li, Q., Wang, B. \harvardand\ Qin, J.
  \harvardyearleft 2024\harvardyearright , `Dialoguellm: Context and emotion
  knowledge-tuned large language models for emotion recognition in
  conversations'.

\end{thebibliography}
