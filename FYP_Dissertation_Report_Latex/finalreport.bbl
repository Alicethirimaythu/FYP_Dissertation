\begin{thebibliography}{xx}

\harvarditem{{\em akrasia, n.}}{n.d.}{oed:akrasia}
{\em akrasia, n.}  \harvardyearleft n.d.\harvardyearright .
\newline\harvardurl{https://www.oed.com/view/Entry/240257?redirectedFrom=akrasia}

\harvarditem[Ameer et~al.]{Ameer, Bölücü, Siddiqui, Can, Sidorov
  \harvardand\ Gelbukh}{2023}{AMEER2023118534}
Ameer, I., Bölücü, N., Siddiqui, M. H.~F., Can, B., Sidorov, G. \harvardand\
  Gelbukh, A.  \harvardyearleft 2023\harvardyearright , `Multi-label emotion
  classification in texts using transfer learning', {\em Expert Systems with
  Applications} {\bf 213},~118534.
\newline\harvardurl{https://www.sciencedirect.com/science/article/pii/S0957417422016098}

\harvarditem{Amrullah}{2023}{Amrullah_2023}
Amrullah, A.  \harvardyearleft 2023\harvardyearright , `Classifying emotions in
  sentence text using neural networks'.
\newline\harvardurl{https://www.analyticsvidhya.com/blog/2023/05/classifying-emotions-in-sentence-text-using-neural-networks/#:~:text=The%20main%20objective%20of%20doing,accuracy%20of%20machine%20translation%20systems.}

\harvarditem{Bhadresh}{2023{\em a}}{Bhadresh_2023_distilbert}
Bhadresh, S.  \harvardyearleft 2023{\em a}\harvardyearright ,
  `Bhadresh-savani/distilbert-base-uncased-emotion · hugging face'.
\newline\harvardurl{https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion}

\harvarditem{Bhadresh}{2023{\em b}}{Bhadresh_2023_roberta}
Bhadresh, S.  \harvardyearleft 2023{\em b}\harvardyearright ,
  `Bhadresh-savani/roberta-base-emotion · hugging face'.
\newline\harvardurl{https://huggingface.co/bhadresh-savani/roberta-base-emotion}

\harvarditem{Company}{2024}{McKinsey_2024}
Company, M.~.  \harvardyearleft 2024\harvardyearright , `What is prompt
  engineering?'.
\newline\harvardurl{https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering}

\harvarditem{Deci}{2024}{Deci_2024}
Deci  \harvardyearleft 2024\harvardyearright , `Model quantization and
  quantization-aware training: Ultimate guide'.
\newline\harvardurl{https://deci.ai/quantization-and-quantization-aware-training/#:~:text=What%20is%20Model%20Quantization%3F,%2Dbit%20or%208%2Dbit.}

\harvarditem[Demszky et~al.]{Demszky, Movshovitz-Attias, Ko, Cowen, Nemade
  \harvardand\ Ravi}{2020}{Demszky_Movshovitz-Attias_Ko_Cowen_Nemade_Ravi_2020}
Demszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G. \harvardand\
  Ravi, S.  \harvardyearleft 2020\harvardyearright , `Goemotions: A dataset of
  fine-grained emotions'.
\newline\harvardurl{https://arxiv.org/abs/2005.00547}

\harvarditem{Dettmers}{n.d.}{Dettmers}
Dettmers, T.  \harvardyearleft n.d.\harvardyearright , `Bitsandbytes'.
\newline\harvardurl{https://huggingface.co/docs/bitsandbytes/main/en/index}

\harvarditem{Hashemi-Pour \harvardand\
  Lutkevich}{2024}{Hashemi-Pour_Lutkevich_2024}
Hashemi-Pour, C. \harvardand\ Lutkevich, B.  \harvardyearleft
  2024\harvardyearright , `What is the bert language model?: Definition from
  techtarget.com'.
\newline\harvardurl{https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection.}

\harvarditem{HuggingFace}{n.d.{\em a}}{Meta_roberta}
HuggingFace  \harvardyearleft n.d.{\em a}\harvardyearright ,
  `Facebookai/roberta-base · hugging face'.
\newline\harvardurl{https://huggingface.co/FacebookAI/roberta-base}

\harvarditem{HuggingFace}{n.d.{\em b}}{Hugging_Face}
HuggingFace  \harvardyearleft n.d.{\em b}\harvardyearright , `Huggingface
  homepage'.
\newline\harvardurl{https://huggingface.co/}

\harvarditem{Huggingface}{n.d.{\em c}}{Huggingface_peft}
Huggingface  \harvardyearleft n.d.{\em c}\harvardyearright , `Huggingface/peft:
  State-of-the-art parameter-efficient fine-tuning.'.
\newline\harvardurl{https://github.com/huggingface/peft}

\harvarditem{HuggingFace}{n.d.{\em d}}{HuggingFace_lora}
HuggingFace  \harvardyearleft n.d.{\em d}\harvardyearright , `Lora'.
\newline\harvardurl{https://huggingface.co/docs/diffusers/main/en/training/lora}

\harvarditem{HuggingFace}{n.d.{\em e}}{openai-community/gpt2}
HuggingFace  \harvardyearleft n.d.{\em e}\harvardyearright ,
  `openai-community/gpt2 · hugging face'.
\newline\harvardurl{https://huggingface.co/openai-community/gpt2}

\harvarditem{IBM}{2024}{IBM_2024}
IBM  \harvardyearleft 2024\harvardyearright , `What is natural language
  processing?'.
\newline\harvardurl{https://www.ibm.com/topics/natural-language-processing}

\harvarditem{Iraqi}{2023}{Iraqi_2023}
Iraqi, M.  \harvardyearleft 2023\harvardyearright , `Comparing the performance
  of llms: A deep dive into roberta, llama-2, and mistral-7b for disaster...'.
\newline\harvardurl{https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a}

\harvarditem{Jorge}{2023}{Jorge_2023}
Jorge, L.  \harvardyearleft 2023\harvardyearright , `Roberta vs. gpt: A
  comprehensive comparison of state-of-the-art language models, with expert
  insights from cronj'.
\newline\harvardurl{https://medium.com/@livajorge7/roberta-vs-86ee82a44969#:~:text=Pretraining%20Objectives%3A%20RoBERTa%20is%20pretrained,masked%20words%20in%20a%20sentence.}

\harvarditem{Krugmann \harvardand\ Hartmann}{2024}{Krugmann_Hartmann_2024}
Krugmann, J.~O. \harvardand\ Hartmann, J.  \harvardyearleft
  2024\harvardyearright , `Sentiment analysis in the age of generative ai -
  customer needs and solutions'.
\newline\harvardurl{https://link.springer.com/article/10.1007/s40547-024-00143-4#Tab1}

\harvarditem{Meta}{2023}{Meta_2023}
Meta  \harvardyearleft 2023\harvardyearright , `Meta-llama/llama-2-7b-hf ·
  hugging face'.
\newline\harvardurl{https://huggingface.co/meta-llama/Llama-2-7b-hf}

\harvarditem{Meta}{2024}{Meta_guideline_2024}
Meta  \harvardyearleft 2024\harvardyearright , `Responsible use guide for llama
  2'.
\newline\harvardurl{https://llama.meta.com/responsible-use-guide}

\harvarditem{Mudadla}{2023}{Mudadla_2023}
Mudadla, S.  \harvardyearleft 2023\harvardyearright , `Difference between
  trainer class and sfttrainer (supervised fine tuning trainer) in hugging
  face?'.
\newline\harvardurl{https://medium.com/@sujathamudadla1213/difference-between-trainer-class-and-sfttrainer-supervised-fine-tuning-trainer-in-hugging-face-d295344d73f7}

\harvarditem{Nedilko}{n.d.}{Nedilko}
Nedilko, A.  \harvardyearleft n.d.\harvardyearright , `Generative pretrained
  transformers for emotion detection in a code-switching setting'.
\newline\harvardurl{https://aclanthology.org/2023.wassa-1.61/}

\harvarditem{OpenAI}{2019}{OpenAI_policy_2019}
OpenAI  \harvardyearleft 2019\harvardyearright .
\newline\harvardurl{https://openai.com/index/better-language-models}

\harvarditem{Pandey}{2021}{Pandey_2021}
Pandey, P.  \harvardyearleft 2021\harvardyearright , `Emotion dataset for
  emotion recognition tasks'.
\newline\harvardurl{https://www.kaggle.com/datasets/parulpandey/emotion-dataset/data}

\harvarditem{patrickvonplaten}{n.d.}{patrickvonplaten}
patrickvonplaten  \harvardyearleft n.d.\harvardyearright ,
  `Microsoft/minilm-l12-h384-uncased · hugging face'.
\newline\harvardurl{https://huggingface.co/microsoft/MiniLM-L12-H384-uncased}

\harvarditem[Saravia et~al.]{Saravia, Liu, Huang, Wu \harvardand\
  Chen}{2018}{saravia-etal-2018-carer}
Saravia, E., Liu, H.-C.~T., Huang, Y.-H., Wu, J. \harvardand\ Chen, Y.-S.
  \harvardyearleft 2018\harvardyearright , {CARER}: Contextualized affect
  representations for emotion recognition, {\em in} `Proceedings of the 2018
  Conference on Empirical Methods in Natural Language Processing', Association
  for Computational Linguistics, Brussels, Belgium, pp.~3687--3697.
\newline\harvardurl{https://www.aclweb.org/anthology/D18-1404}

\harvarditem{Sharma}{2022}{Sharma_2022}
Sharma, D.  \harvardyearleft 2022\harvardyearright , `A gentle introduction to
  roberta'.
\newline\harvardurl{https://www.analyticsvidhya.com/blog/2022/10/a-gentle-introduction-to-roberta/}

\harvarditem{Uwa}{2023}{Uwa_2023}
Uwa  \harvardyearleft 2023\harvardyearright , `Science of emotion: The basics
  of emotional psychology: Uwa'.
\newline\harvardurl{https://online.uwa.edu/news/emotional-psychology/}

\harvarditem[Vaswani et~al.]{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser \harvardand\
  Polosukhin}{2023}{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2023}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L. \harvardand\ Polosukhin, I.  \harvardyearleft
  2023\harvardyearright , `Attention is all you need'.
\newline\harvardurl{https://arxiv.org/abs/1706.03762}

\harvarditem[Wang et~al.]{Wang, Wei, Dong, Bao, Yang \harvardand\
  Zhou}{2020}{Wang_Wei_Dong_Bao_Yang_Zhou_2020}
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N. \harvardand\ Zhou, M.
  \harvardyearleft 2020\harvardyearright , `Minilm: Deep self-attention
  distillation for task-agnostic compression of pre-trained transformers'.
\newline\harvardurl{https://arxiv.org/abs/2002.10957}

\harvarditem[Zhang et~al.]{Zhang, Wang, Wu, Tiwari, Li, Wang \harvardand\
  Qin}{2024}{zhang2024dialoguellm}
Zhang, Y., Wang, M., Wu, Y., Tiwari, P., Li, Q., Wang, B. \harvardand\ Qin, J.
  \harvardyearleft 2024\harvardyearright , `Dialoguellm: Context and emotion
  knowledge-tuned large language models for emotion recognition in
  conversations'.

\end{thebibliography}
