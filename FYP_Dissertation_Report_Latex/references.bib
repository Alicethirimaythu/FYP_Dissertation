# $Id: references.bib 1789 2010-09-28 16:30:23Z jabriffa $

# This file contains bibliography entries in BibTeX format. Examples are
# provided of the more common entry types.


@MISC{oed:akrasia,
  title     = {akrasia, n.},
  booktitle = {OED Online},
  date      = {2020-06},
  publisher = {Oxford University Press},
  url       = {https://www.oed.com/view/Entry/240257?redirectedFrom=akrasia},
  urldate   = {2023-03-03}
}

@article{AMEER2023118534,
title = {Multi-label emotion classification in texts using transfer learning},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118534},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118534},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016098},
author = {Iqra Ameer and Necva Bölücü and Muhammad Hammad Fahim Siddiqui and Burcu Can and Grigori Sidorov and Alexander Gelbukh},
keywords = {Multi-label emotion classification, Bi-LSTM, Transformer Networks, Attention mechanism, Social media},
abstract = {Social media is a widespread platform that provides a massive amount of user-generated content that can be mined to reveal the emotions of social media users. This has many potential benefits, such as getting a sense of people’s pulse on various events or news. Emotion classification from social media posts is challenging, especially when it comes to detecting multiple emotions from a short piece of text, as in multi-label classification problem. Most of the previous work on emotion detection has focused on deep neural networks such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM) Networks. However, none of them has utilized multiple attention mechanisms and Recurrent Neural Networks (i.e., specialized attention networks for each emotion) nor utilized the recently introduced Transformer Networks such as XLNet, DistilBERT, and RoBERTa for the task of classifying emotions with multiple labels. The proposed multiple attention mechanism reveals the contribution of each word on each emotion, which has not been investigated before. In this study, we investigate both the use of LSTMs and the fine-tuning of Transformer Networks through Transfer Learning along with a single-attention network and a multiple-attention network for multi-label emotion classification. The experimental results show that our novel transfer learning models using pre-trained transformers with and without multiple attention mechanisms were able to outperform the current state-of-the-art accuracy (58.8% - Baziotis et al., 2018) in the SemEval-2018 Task-1C dataset. Our best-performing RoBERTa-MA (RoBERTa-Multi-attention) model outperformed the state-of-the-art and achieved 62.4% accuracy (3.6% gain over the state-of-the-art) on the challenging SemEval-2018 E-c: Detecting Emotions (multi-label classification) dataset for English. Moreover, the XLNet-MA (XLNet-Multi-attention) model outperformed the other proposed models by achieving 45.6% accuracy on the Ren-CECps dataset for Chinese.}
}

@MISC{nlp,
  title     = {What is natural language processing (NLP)?},
  publisher = {IBM},
  url       = {https://www.ibm.com/topics/natural-language-processing},
  urldate   = {2023-03-03}
}

@misc{Liu_Shi_Zhou_Liu_Yin_Yin_Zheng_2023, 
title={Emotion classification for short texts: An improved multi-label method}, 
url={https://www.nature.com/articles/s41599-023-01816-6#citeas}, 
journal={Nature News}, 
publisher={Nature Publishing Group}, 
author={Liu, Xuan and Shi, Tianyi and Zhou, Guohui and Liu, Mingzhe and Yin, Zhengtong and Yin, Lirong and Zheng, Wenfeng}, 
year={2023}, 
month={Jun}} 

@inproceedings{saravia-etal-2018-carer,
   title = "{CARER}: Contextualized Affect Representations for Emotion Recognition",
   author = "Saravia, Elvis  and
     Liu, Hsien-Chi Toby  and
     Huang, Yen-Hao  and
     Wu, Junlin  and
     Chen, Yi-Shin",
   booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
   month = oct # "-" # nov,
   year = "2018",
   address = "Brussels, Belgium",
   publisher = "Association for Computational Linguistics",
   url = "https://www.aclweb.org/anthology/D18-1404",
   doi = "10.18653/v1/D18-1404",
   pages = "3687--3697",
   abstract = "Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.",
}

@misc{Hashemi-Pour_Lutkevich_2024, 
title={What is the bert language model?: Definition from techtarget.com}, 
url={https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection.}, 
journal={Enterprise AI}, 
publisher={TechTarget}, 
author={Hashemi-Pour, Cameron and Lutkevich, Ben}, 
year={2024}, 
month={Feb}} 

@misc{Sharma_2022, 
title={A gentle introduction to Roberta}, 
url={https://www.analyticsvidhya.com/blog/2022/10/a-gentle-introduction-to-roberta/}, 
journal={Analytics Vidhya}, 
author={Sharma, Drishti}, 
year={2022}, 
month={Nov}} 

@misc{Wang_Wei_Dong_Bao_Yang_Zhou_2020, 
title={MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers}, url={https://arxiv.org/abs/2002.10957}, 
journal={arXiv.org}, 
author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming}, 
year={2020}, 
month={Apr}} 

@misc{Iraqi_2023, title={Comparing the performance of llms: A deep dive into Roberta, llama-2, and mistral-7b for disaster...}, url={https://medium.com/@mehdi.iraqui/comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-and-mistral-for-disaster-tweets-8069e717548a}, journal={Medium}, publisher={Medium}, author={Iraqi, Mehdi}, year={2023}, month={Oct}} 

@misc{Krugmann_Hartmann_2024, title={Sentiment analysis in the age of Generative AI - Customer Needs and Solutions}, url={https://link.springer.com/article/10.1007/s40547-024-00143-4#Tab1}, journal={SpringerLink}, publisher={Springer US}, author={Krugmann, Jan Ole and Hartmann, Jochen}, year={2024}, month={Mar}} 

@misc{Jorge_2023, title={RoBERTa vs. GPT: A Comprehensive Comparison of State-of-the-Art Language Models, with Expert Insights from CronJ}, url={https://medium.com/@livajorge7/roberta-vs-86ee82a44969#:~:text=Pretraining%20Objectives%3A%20RoBERTa%20is%20pretrained,masked%20words%20in%20a%20sentence.}, journal={Medium}, publisher={Medium}, author={Jorge, Liva}, year={2023}, month={Mar}} 

@misc{Nedilko, title={Generative pretrained transformers for emotion detection in a code-switching setting}, url={https://aclanthology.org/2023.wassa-1.61/}, journal={ACL Anthology}, author={Nedilko, Andrew}} 

@misc{Uwa_2023, title={Science of emotion: The Basics of Emotional Psychology: UWA}, url={https://online.uwa.edu/news/emotional-psychology/}, journal={UWA Online}, author={Uwa}, year={2023}, month={Sep}} 








@CONFERENCE{bsw10icc,
   AUTHOR  = "Johann A. Briffa and Hans Georg Schaathun and Stephan Wesemeyer",
   TITLE   = "An Improved Decoding Algorithm for the {Davey-MacKay} Construction",
   BOOKTITLE = "Proc.\ {IEEE} Intern.\ Conf.\ on Commun.",
   ADDRESS = "Cape Town, South Africa",
   MONTH   = may # "~23--27,",
   YEAR    = 2010
   }

@ARTICLE{gamal,
   AUTHOR   = "Abbas A. el Gamal and Lane A. Hemachandra and Itzhak Shperling and Victor K. Wei",
   TITLE = "Using Simulated Annealing to Design Good Codes",
   JOURNAL  = "IEEE Transactions on Information Theory",
   YEAR  = 1987,
   MONTH = jan,
   VOLUME   = 33,
   NUMBER   = 1,
   PAGES = "116--123"
   }

@MISC{farrell,
   AUTHOR   = "P. G. Farrell",
   TITLE = "Notes on Source Coding",
   MONTH = feb,
   YEAR  = 1992,
   NOTE  = "University of Manchester"
   }

@PHDTHESIS{tunstall,
   AUTHOR   = "B. P. Tunstall",
   TITLE = "Synthesis of Noiseless Compression Codes",
   SCHOOL   = "Georgia Institute of Technology",
   YEAR  = 1968
   }

@BOOK{press,
   AUTHOR   = "William H. Press and Saul A. Teukolsky and William T. Vetterling and Brian P. Flannery",
   TITLE = "Numerical Recipes in {C}: The Art of Scientific Computing",
   PUBLISHER   = "Cambridge Universty Press",
   YEAR  = 1992,
   EDITION  = "Second"
   }

@MANUAL{netpbm,
   AUTHOR = "Bryan Henderson",
   TITLE = "Netpbm",
   URL = "http://netpbm.sourceforge.net/doc/",
   MONTH = feb # "~22nd,",
   YEAR = 2009
   }

@MANUAL{cuda-pg,
   ORGANIZATION    = "NVIDIA Corporation",
   TITLE   = "NVIDIA CUDA Programming Guide",
   NOTE    = "Version 2.3",
   MONTH   = jul # "~1st,",
   YEAR    = 2009
   }

@TECHREPORT{bw1994,
    author = {Michael Burrows and David J. Wheeler},
    title = {A Block-Sorting Lossless Data Compression Algorithm},
    institution = {Digital SRC Research Report},
    year = {1994}
}
